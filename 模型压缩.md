[大模型学习路线（5）—— 大模型压缩（量化、剪枝、蒸馏、低秩分解），推理（vllm）_大模型量化和vllm区别-CSDN博客](https://blog.csdn.net/qq_51175703/article/details/138320834)

## 模型压缩

模型压缩通常处于机器学习模型训练和生产部署之间的阶段。它在模型训练完成后，准备将模型部署到目标环境之前进行。

本文针对第二部分量化、剪枝、蒸馏、低秩分解不同的模型压缩算法分类进行了详细分析。具体参考了ZOMI酱老师的课程：[AISystem/04Inference at main · chenzomi12/AISystem (github.com)](https://github.com/chenzomi12/AISystem/tree/main/04Inference)，该文章是ZOMI酱老师课程的学习笔记。



### 模型量化（quantization）

#### 量化概念

型量化是一种将神经网络的浮点算法转换为低比特定点计算的技术（也可以理解为数据映射），它主要适用于需要在资源受限的设备上部署模型的场景，如移动设备、嵌入式系统等。

#### 模型量化优点

（1）提高模型获取参数的时间；（2）参数压缩，加速模型推理时间；（3）减少内存访问开销，节省内存，减少芯片消耗面积，降低能耗。

#### 什么情况下应该/不应该使用模型量化

用场景：（1）资源受限的环境；（2）有实时性要求；（3）能耗敏感的应用；（4）有硬件兼容性；（5）模型部署的便捷性。

不适用场景：（1）有高精度要求的任务；（2）模型仍在迭代开发；（3）缺乏量化支持的硬件；（4）对开发成本敏感的项目；（5）研究和实验阶段。

#### 落地挑战

（1）量化方法：线性量化对数据分布的描述不精确；（2）低比特——从 16 bits ->4 bits 比特数越低，精度损失越大；（3）任务：分类、检测、识别中任务越复杂，精度损失越大；（4）大小：模型越小，精度损失越大。



#### 量化方法

![image-20241202151752634](模型压缩.assets/image-20241202151752634.png)





















