## 分布式训练

[[理论+实操\] MONAI&PyTorch 如何进行分布式训练，详细介绍DP和DDP-CSDN博客](https://blog.csdn.net/u014264373/article/details/127011482)

### 为什么要使用分布式训练

于懒癌星人，单卡训练能解决的问题，再多卡给我，我都懒得用。但是对于资深炼丹师，分布式训练带来的收益很大，可以加速模型的训练、调参的节奏、以及版本的迭代更新等~

当你遇到以下情况，可以考虑使用分布式训练（使用多张卡，或者多台服务器）

在理想的batc_size下，单卡训练 out of memory
使用单卡虽然能run, 但是速度非常慢，耗时。
数据量太大，需要利用所有资源满足训练。
模型太大，单机内存不足
等…

### 分布式训练有哪些方法

分布式训练策略按照并行方式不同，可以简单的分为**数据并行**和**模型并行**两种方式。[原文链接](https://cloud.tencent.com/developer/news/841792)

1️⃣ 数据并行
数据并行是指在不同的 GPU 上都 copy 保存一份模型的副本，然后将不同的数据分配到不同的 GPU 上进行计算，最后将所有 GPU 计算的结果进行合并，从而达到加速模型训练的目的。由于数据并行会涉及到把不同 GPU 的计算结果进行合并然后再更新模型，根据跟新方式不同，又可以分为同步更新和异步更新

2️⃣ 模型并行
分布式训练中的模型并行是指将整个神经网络模型拆解分布到不同的 GPU 中，不同的 GPU 负责计算网络模型中的不同部分。这通常是在网络模型很大很大、单个 GPU 的显存已经完全装不下整体网络的情况下才会采用。



### 基于 Pytorch 的分布式训练方法

 Pytorch 中为我们提供了两种多 GPU 的分布式训练方案： torch.nn.DataParallel（DP）和 torch.nn.parallel.Distributed Data Parallel（DDP)

#### DP(DataParallel)

[原文链接](https://blog.csdn.net/laizi_laizi/article/details/115299263)

优点：修改的代码量最少，只要像这样model = nn.DataParallel(model)包裹一下你的模型就行了
缺点：只适用单机多卡，不适用多机多卡；性能不如DDP; DP使用单进程，目前官方已经不推荐。
如果觉得 DDP 很难，可以采用这种方式。示例用法

```python
import monai
import torch.nn as nn
import os

os.environ("CUDA_VISIBLE_DEVICES") = '0,1' 
# 用哪些卡，就写哪些，也可以在命令行运行的时候指定可见GPU
# $: export CUDA_VISIBLE_DEVICES=0,1 python train.py
device = torch.device('cuda' if torch.cuda.is_available () else 'cpu')
model = monai.networks.nets.UNet().to(device)
model = nn.DataParallel(model)

```







































