## åˆ†å¸ƒå¼è®­ç»ƒ

[[ç†è®º+å®æ“\] MONAI&PyTorch å¦‚ä½•è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯¦ç»†ä»‹ç»DPå’ŒDDP-CSDNåšå®¢](https://blog.csdn.net/u014264373/article/details/127011482)

### ä¸ºä»€ä¹ˆè¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ

äºæ‡’ç™Œæ˜Ÿäººï¼Œå•å¡è®­ç»ƒèƒ½è§£å†³çš„é—®é¢˜ï¼Œå†å¤šå¡ç»™æˆ‘ï¼Œæˆ‘éƒ½æ‡’å¾—ç”¨ã€‚ä½†æ˜¯å¯¹äºèµ„æ·±ç‚¼ä¸¹å¸ˆï¼Œåˆ†å¸ƒå¼è®­ç»ƒå¸¦æ¥çš„æ”¶ç›Šå¾ˆå¤§ï¼Œå¯ä»¥åŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒã€è°ƒå‚çš„èŠ‚å¥ã€ä»¥åŠç‰ˆæœ¬çš„è¿­ä»£æ›´æ–°ç­‰~

å½“ä½ é‡åˆ°ä»¥ä¸‹æƒ…å†µï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆä½¿ç”¨å¤šå¼ å¡ï¼Œæˆ–è€…å¤šå°æœåŠ¡å™¨ï¼‰

åœ¨ç†æƒ³çš„batc_sizeä¸‹ï¼Œå•å¡è®­ç»ƒ out of memory
ä½¿ç”¨å•å¡è™½ç„¶èƒ½run, ä½†æ˜¯é€Ÿåº¦éå¸¸æ…¢ï¼Œè€—æ—¶ã€‚
æ•°æ®é‡å¤ªå¤§ï¼Œéœ€è¦åˆ©ç”¨æ‰€æœ‰èµ„æºæ»¡è¶³è®­ç»ƒã€‚
æ¨¡å‹å¤ªå¤§ï¼Œå•æœºå†…å­˜ä¸è¶³
ç­‰â€¦

### åˆ†å¸ƒå¼è®­ç»ƒæœ‰å“ªäº›æ–¹æ³•

åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥æŒ‰ç…§å¹¶è¡Œæ–¹å¼ä¸åŒï¼Œå¯ä»¥ç®€å•çš„åˆ†ä¸º**æ•°æ®å¹¶è¡Œ**å’Œ**æ¨¡å‹å¹¶è¡Œ**ä¸¤ç§æ–¹å¼ã€‚[åŸæ–‡é“¾æ¥](https://cloud.tencent.com/developer/news/841792)

1ï¸âƒ£ æ•°æ®å¹¶è¡Œ
æ•°æ®å¹¶è¡Œæ˜¯æŒ‡åœ¨ä¸åŒçš„ GPU ä¸Šéƒ½ copy ä¿å­˜ä¸€ä»½æ¨¡å‹çš„å‰¯æœ¬ï¼Œç„¶åå°†ä¸åŒçš„æ•°æ®åˆ†é…åˆ°ä¸åŒçš„ GPU ä¸Šè¿›è¡Œè®¡ç®—ï¼Œæœ€åå°†æ‰€æœ‰ GPU è®¡ç®—çš„ç»“æœè¿›è¡Œåˆå¹¶ï¼Œä»è€Œè¾¾åˆ°åŠ é€Ÿæ¨¡å‹è®­ç»ƒçš„ç›®çš„ã€‚ç”±äºæ•°æ®å¹¶è¡Œä¼šæ¶‰åŠåˆ°æŠŠä¸åŒ GPU çš„è®¡ç®—ç»“æœè¿›è¡Œåˆå¹¶ç„¶åå†æ›´æ–°æ¨¡å‹ï¼Œæ ¹æ®è·Ÿæ–°æ–¹å¼ä¸åŒï¼Œåˆå¯ä»¥åˆ†ä¸ºåŒæ­¥æ›´æ–°å’Œå¼‚æ­¥æ›´æ–°

2ï¸âƒ£ æ¨¡å‹å¹¶è¡Œ
åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ¨¡å‹å¹¶è¡Œæ˜¯æŒ‡å°†æ•´ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹æ‹†è§£åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸­ï¼Œä¸åŒçš„ GPU è´Ÿè´£è®¡ç®—ç½‘ç»œæ¨¡å‹ä¸­çš„ä¸åŒéƒ¨åˆ†ã€‚è¿™é€šå¸¸æ˜¯åœ¨ç½‘ç»œæ¨¡å‹å¾ˆå¤§å¾ˆå¤§ã€å•ä¸ª GPU çš„æ˜¾å­˜å·²ç»å®Œå…¨è£…ä¸ä¸‹æ•´ä½“ç½‘ç»œçš„æƒ…å†µä¸‹æ‰ä¼šé‡‡ç”¨ã€‚



### åŸºäº Pytorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•

 Pytorch ä¸­ä¸ºæˆ‘ä»¬æä¾›äº†ä¸¤ç§å¤š GPU çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ¡ˆï¼š torch.nn.DataParallelï¼ˆDPï¼‰å’Œ torch.nn.parallel.Distributed Data Parallelï¼ˆDDP)

#### DP(DataParallel)

[åŸæ–‡é“¾æ¥](https://blog.csdn.net/laizi_laizi/article/details/115299263)

ä¼˜ç‚¹ï¼šä¿®æ”¹çš„ä»£ç é‡æœ€å°‘ï¼Œåªè¦åƒè¿™æ ·model = nn.DataParallel(model)åŒ…è£¹ä¸€ä¸‹ä½ çš„æ¨¡å‹å°±è¡Œäº†
ç¼ºç‚¹ï¼šåªé€‚ç”¨å•æœºå¤šå¡ï¼Œä¸é€‚ç”¨å¤šæœºå¤šå¡ï¼›æ€§èƒ½ä¸å¦‚DDP; DPä½¿ç”¨å•è¿›ç¨‹ï¼Œç›®å‰å®˜æ–¹å·²ç»ä¸æ¨èã€‚
å¦‚æœè§‰å¾— DDP å¾ˆéš¾ï¼Œå¯ä»¥é‡‡ç”¨è¿™ç§æ–¹å¼ã€‚ç¤ºä¾‹ç”¨æ³•

```python
import monai
import torch.nn as nn
import os

os.environ("CUDA_VISIBLE_DEVICES") = '0,1' 
# ç”¨å“ªäº›å¡ï¼Œå°±å†™å“ªäº›ï¼Œä¹Ÿå¯ä»¥åœ¨å‘½ä»¤è¡Œè¿è¡Œçš„æ—¶å€™æŒ‡å®šå¯è§GPU
# $: export CUDA_VISIBLE_DEVICES=0,1 python train.py
device = torch.device('cuda' if torch.cuda.is_available () else 'cpu')
model = monai.networks.nets.UNet().to(device)
model = nn.DataParallel(model)

```

é€šè¿‡ä¸¤ç§æ–¹å¼å¯ä»¥æŒ‡å®šéœ€è¦ä½¿ç”¨çš„GPUï¼Œç¬¬ä¸€ç§æ˜¯åœ¨ä»£ç é‡Œè®¾ç½®`os.environ`, ç¬¬äºŒç§æ˜¯åœ¨ç»ˆç«¯è¿è¡Œä»£ç å‰ï¼ŒåŠ ä¸€å¥`export CUDA_VISIBLE_DEVICES=0,1`ã€‚æŒ‰ç…§è‡ªå·±çš„ä¹ æƒ¯æ¥å°±è¡Œã€‚

å¦‚æœéœ€è¦è·‘æ¥çœ‹çœ‹æ•ˆæœï¼Œå¯ä»¥æ‰§è¡Œä¸‹é¢çš„ä»£ç ï¼Œå®Œæ•´ç‰ˆ

```python
import argparse
import os
import sys
from glob import glob

import nibabel as nib
import numpy as np
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

import monai
from monai.data import DataLoader, Dataset, create_test_image_3d, DistributedSampler
from monai.transforms import (
    AsChannelFirstd,
    Compose,
    LoadImaged,
    RandCropByPosNegLabeld,
    RandRotate90d,
    ScaleIntensityd,
    EnsureTyped,
)


def train(args):
    if not os.path.exists(args.dir):
        # create 40 random image, mask paris for training
        print(f"generating synthetic data to {args.dir} (this may take a while)")
        os.makedirs(args.dir)
        # set random seed to generate same random data for every node
        np.random.seed(seed=0)
        for i in range(200):
            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(args.dir, f"img{i:d}.nii.gz"))
            n = nib.Nifti1Image(seg, np.eye(4))
            nib.save(n, os.path.join(args.dir, f"seg{i:d}.nii.gz"))

    images = sorted(glob(os.path.join(args.dir, "img*.nii.gz")))
    segs = sorted(glob(os.path.join(args.dir, "seg*.nii.gz")))
    train_files = [{"img": img, "seg": seg} for img, seg in zip(images, segs)]

    # define transforms for image and segmentation
    train_transforms = Compose(
        [
            LoadImaged(keys=["img", "seg"]),
            AsChannelFirstd(keys=["img", "seg"], channel_dim=-1),
            ScaleIntensityd(keys="img"),
            RandCropByPosNegLabeld(
                keys=["img", "seg"], label_key="seg", spatial_size=[96, 96, 96], pos=1, neg=1, num_samples=4
            ),
            RandRotate90d(keys=["img", "seg"], prob=0.5, spatial_axes=[0, 2]),
            EnsureTyped(keys=["img", "seg"]),
        ]
    )

    # create a training data loader
    train_ds = Dataset(data=train_files, transform=train_transforms)
    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training
    train_loader = DataLoader(
        train_ds,
        batch_size=10,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
    )

    # create UNet, DiceLoss and Adam optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = monai.networks.nets.UNet(
        spatial_dims=3,
        in_channels=1,
        out_channels=1,
        channels=(16, 32, 64, 128, 256),
        strides=(2, 2, 2, 2),
        num_res_units=2,
    ).to(device)
    model = torch.nn.DataParallel(model)
    loss_function = monai.losses.DiceLoss(sigmoid=True).to(device)
    optimizer = torch.optim.Adam(model.parameters(), 1e-3)

    # start a typical PyTorch training
    epoch_loss_values = list()
    for epoch in range(5):
        print("-" * 10)
        print(f"epoch {epoch + 1}/{5}")
        model.train()
        epoch_loss = 0
        step = 0
        for batch_data in train_loader:
            step += 1
            inputs, labels = batch_data["img"].to(device), batch_data["seg"].to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            epoch_len = len(train_ds) // train_loader.batch_size
            print(f"{step}/{epoch_len}, train_loss: {loss.item():.4f}")
        epoch_loss /= step
        epoch_loss_values.append(epoch_loss)
        print(f"epoch {epoch + 1} average loss: {epoch_loss:.4f}")
    print(f"train completed, epoch losses: {epoch_loss_values}")
    torch.save(model.state_dict(), "final_model.pth")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--dir", default="./testdata", type=str, help="directory to create random data")
    args = parser.parse_args()

    train(args=args)

if __name__ == "__main__":
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = '0,1'
    main()

```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†2å—GPUï¼Œè¿è¡Œçš„æ—¶å€™å¯ä»¥çœ‹ä¸‹ä¸¤å—GPUçš„ä½¿ç”¨æƒ…å†µ.

![image-20241225163915727](æ¨¡å‹è®­ç»ƒ.assets/image-20241225163915727.png)



#### DDP(DistributedDataParallel)

æ¬¡é‡ç‚¹ä»‹ç»ï¼šâ­ï¸â­ï¸â­ï¸â­ï¸â­ï¸

ä¸ DP æ¨¡å¼ä¸åŒï¼ŒDDP æ¨¡å¼æœ¬èº«æ˜¯ä¸ºå¤šæœºå¤šå¡è®¾è®¡çš„ï¼Œå½“ç„¶åœ¨å•æœºå¤šå¡çš„æƒ…å†µä¸‹ä¹Ÿå¯ä»¥ä½¿ç”¨ã€‚DDP é‡‡ç”¨çš„æ˜¯ all-reduce æ¶æ„ï¼ŒåŸºæœ¬è§£å†³äº† PS æ¶æ„ä¸­é€šä¿¡æˆæœ¬ä¸ GPU çš„æ•°é‡çº¿æ€§ç›¸å…³çš„é—®é¢˜ã€‚è™½ç„¶åœ¨å•æœºå¤šå¡æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨ DP æ¨¡å¼ï¼Œä½†æ˜¯ä½¿ç”¨ DDP é€šå¸¸ä¼šæ¯” DP æ¨¡å¼å¿«ä¸€äº›ï¼Œå› æ­¤ DDP æ¨¡å¼ä¹Ÿæ˜¯å®˜æ–¹æ¨èå¤§å®¶ä½¿ç”¨çš„æ–¹å¼ã€‚

DDPä¸ºåŸºäºtorch.distributedçš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç»“æ„ï¼Œå·¥ä½œæœºåˆ¶ä¸ºï¼šåœ¨batchç»´åº¦ä¸Šå¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ï¼Œå°†è¾“å…¥çš„æ•°æ®åˆ†é…åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆGPUï¼‰ä¸Šï¼Œä»è€Œå°†ç¨‹åºçš„æ¨¡å‹å¹¶è¡ŒåŒ–ã€‚å¯¹åº”çš„ï¼Œæ¯ä¸ªGPUä¸Šä¼šå¤åˆ¶ä¸€ä¸ªæ¨¡å‹çš„å‰¯æœ¬ï¼Œè´Ÿè´£å¤„ç†åˆ†é…åˆ°çš„æ•°æ®ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å†å¯¹æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ¢¯åº¦è¿›è¡Œå¹³å‡ã€‚

ç¼ºç‚¹ï¼šä»£ç æ”¹åŠ¨è¾ƒDPå¤šï¼Œå‘è¾ƒå¤šï¼Œéœ€è¦è¯•é”™æ”’ç»éªŒ

DDPçš„å¯åŠ¨æ–¹å¼æœ‰å¾ˆå¤šç§ï¼Œå†…å®¹ä¸Šæ˜¯ç»Ÿä¸€çš„ï¼šéƒ½æ˜¯å¯åŠ¨å¤šè¿›ç¨‹æ¥å®Œæˆè¿ç®—ã€‚

- torch.multiprocessing.spawnï¼šé€‚ç”¨äºå•æœºå¤šå¡
- torch.distributed.launch: å¯ç”¨äºå¤šæœºæˆ–å¤šå¡ã€‚

æ¥ä¸‹æ¥ä»¥`torch.distributed.launch`ä¸ºä¾‹ï¼Œåªéœ€è¦8æ­¥ï¼Œå°†ä½ çš„ä»£ç æ”¹æˆåˆ†å¸ƒå¼è®­ç»ƒã€‚é€‚ç”¨äºå•æœºå¤šå¡ã€‚

##### step 1ï¼š åˆå§‹åŒ–è¿›ç¨‹

```python
import torch.distributed as dist
dist.init_process_group(backend="nccl", init_method="env://")
```

å‚æ•°è§£æï¼š
åœ¨åˆ›å»ºæ¨¡å‹ä¹‹å‰è¿›è¡Œåˆå§‹åŒ–

- backend: åç«¯é€šä¿¡å¯ä»¥é‡‡ç”¨`mpi`, `gloo`,and `nccl`ã€‚å¯¹äºåŸºäº GPU çš„è®­ç»ƒï¼Œå»ºè®®ä½¿ç”¨ `nccl` ä»¥è·å¾—æœ€ä½³æ€§èƒ½.
- init_method: å‘ŠçŸ¥æ¯ä¸ªè¿›ç¨‹å¦‚ä½•å‘ç°å½¼æ­¤ï¼Œå¦‚ä½•ä½¿ç”¨é€šä¿¡åç«¯åˆå§‹åŒ–å’ŒéªŒè¯è¿›ç¨‹ç»„ã€‚ é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæœªæŒ‡å®š init_methodï¼ŒPyTorch å°†ä½¿ç”¨ç¯å¢ƒå˜é‡åˆå§‹åŒ–æ–¹æ³• (env://)ã€‚
  

![image-20241225164448823](æ¨¡å‹è®­ç»ƒ.assets/image-20241225164448823.png)

å›¾ç‰‡å·¦è¾¹ä¸ºå¸¸è§„ä»£ç ï¼Œå³è¾¹ä¸ºDDPä¿®æ”¹åçš„ä»£ç 

##### step 2: åœ¨åˆ›å»ºdataloderå‰åŠ ä¸€ä¸ªsampler

```python
from monai.data import DistributedSampler
 # create a training data loader
train_ds = Dataset(data=train_files, transform=train_transforms)
# create a training data sampler
train_sampler = DistributedSampler(dataset=train_ds, even_divisible=True, shuffle=True)
# use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training
train_loader = DataLoader(
    train_ds,
    batch_size=10,
    shuffle=False,
    num_workers=2,
    pin_memory=True,
    sampler=train_sampler,
)
```

ä¸å¸¸è§„è®­ç»ƒçš„åŒºåˆ«ğŸ‘‡çº¢æ¡†å†…ä¸ºæ–°å¢å†…å®¹

![image-20241225164650071](æ¨¡å‹è®­ç»ƒ.assets/image-20241225164650071.png)

##### step 3: è®¾å®šDevice

```python
device = torch.device(f"cuda:{args.local_rank}")
torch.cuda.set_device(device)
```

è¿™é‡Œæ¶‰åŠåˆ°çš„å‚æ•°åœ¨åé¢(step 7)ç»™å‡º
è¿™ä¸€æ­¥ï¼Œå¸¸è§„çš„ä»£ç ä¹Ÿå¯ä»¥è¿™æ ·å†™ï¼Œä½†å¹³æ—¶ä¸€èˆ¬ç”¨å¦‚ä¸‹æ–¹æ³•

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

##### step 4: ä½¿ç”¨DistributedDataParallelæ¨¡å—åŒ…è£¹æ¨¡å‹

```python
from torch.nn.parallel import DistributedDataParallel
model = monai.networks.nets.UNet(
        spatial_dims=3,
        in_channels=1,
        out_channels=1,
        channels=(16, 32, 64, 128, 256),
        strides=(2, 2, 2, 2),
        num_res_units=2,
    ).to(device)
model = DistributedDataParallel(model, device_ids=[device], output_device=[device])

```

> model.to(device) # è¿™å¥ä¸èƒ½å°‘ï¼Œæœ€å¥½ä¸è¦ç”¨model.cuda()

ä¸å¸¸è§„å¯¹æ¯”

![image-20241225164929615](æ¨¡å‹è®­ç»ƒ.assets/image-20241225164929615.png)

##### step 5: åœ¨epochè®­ç»ƒå‰è®¾ç½®`set_epoch`

```python
train_sampler.set_epoch(epoch)
```

![image-20241225165042791](æ¨¡å‹è®­ç»ƒ.assets/image-20241225165042791.png)

åœ¨åé¢è§£é‡Šä¸ºä»€ä¹ˆè¦è®¾ç½®`set_epoch`

##### step 6: ä¿®æ”¹æ¨¡å‹çš„ä¿å­˜æ–¹å¼

å› ä¸ºæ¯ä¸ªè¿›ç¨‹çš„æ¨¡å‹æ˜¯ä¸€æ ·çš„ï¼Œæˆ‘ä»¬åªç”¨åœ¨æŸä¸€ä¸ªè¿›ç¨‹ä¸Šä¿å­˜æ¨¡å‹å°±è¡Œï¼Œé»˜è®¤ä½¿ç”¨0è¿›ç¨‹ä¿å­˜æ¨¡å‹

```python
if dist.get_rank() == 0:
    # all processes should see same parameters as they all start from same
    # random parameters and gradients are synchronized in backward passes,
    # therefore, saving it in one process is sufficient
    torch.save(model.state_dict(), "final_model.pth")
```

![image-20241225165214684](æ¨¡å‹è®­ç»ƒ.assets/image-20241225165214684.png)

##### step 7: åœ¨mainå‡½æ•°é‡Œé¢æ·»åŠ local_rankå‚æ•°

```python
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--dir", default="./testdata", type=str, help="directory to create random data")
    # must parse the command-line argument: ``--local_rank=LOCAL_PROCESS_RANK``, which will be provided by DDP
    parser.add_argument("--local_rank", type=int, default=0)
    args = parser.parse_args()

    train(args=args)
```

7æ­¥å·²ç»æ”¹å®Œäº†æ‰€æœ‰ä»£ç ï¼Œæ¥ä¸‹æ¥copyå®Œæ•´ä»£ç ï¼Œå°è¯•è·‘é€šğŸ’ªğŸ’ª

**å®Œæ•´ä»£ç å¦‚ä¸‹**

```python
import argparse
import os
import sys
from glob import glob

import nibabel as nib
import numpy as np
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

import monai
from monai.data import DataLoader, Dataset, create_test_image_3d, DistributedSampler
from monai.transforms import (
    AsChannelFirstd,
    Compose,
    LoadImaged,
    RandCropByPosNegLabeld,
    RandRotate90d,
    ScaleIntensityd,
    EnsureTyped,
)


def train(args):
    # disable logging for processes except 0 on every node
    if args.local_rank != 0:
        f = open(os.devnull, "w")
        sys.stdout = sys.stderr = f
    elif not os.path.exists(args.dir):
        # create 40 random image, mask paris for training
        print(f"generating synthetic data to {args.dir} (this may take a while)")
        os.makedirs(args.dir)
        # set random seed to generate same random data for every node
        np.random.seed(seed=0)
        for i in range(100):
            im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)
            n = nib.Nifti1Image(im, np.eye(4))
            nib.save(n, os.path.join(args.dir, f"img{i:d}.nii.gz"))
            n = nib.Nifti1Image(seg, np.eye(4))
            nib.save(n, os.path.join(args.dir, f"seg{i:d}.nii.gz"))

    # initialize the distributed training process, every GPU runs in a process
    dist.init_process_group(backend="nccl", init_method="env://")

    images = sorted(glob(os.path.join(args.dir, "img*.nii.gz")))
    segs = sorted(glob(os.path.join(args.dir, "seg*.nii.gz")))
    train_files = [{"img": img, "seg": seg} for img, seg in zip(images, segs)]

    # define transforms for image and segmentation
    train_transforms = Compose(
        [
            LoadImaged(keys=["img", "seg"]),
            AsChannelFirstd(keys=["img", "seg"], channel_dim=-1),
            ScaleIntensityd(keys="img"),
            RandCropByPosNegLabeld(
                keys=["img", "seg"], label_key="seg", spatial_size=[96, 96, 96], pos=1, neg=1, num_samples=4
            ),
            RandRotate90d(keys=["img", "seg"], prob=0.5, spatial_axes=[0, 2]),
            EnsureTyped(keys=["img", "seg"]),
        ]
    )

    # create a training data loader
    train_ds = Dataset(data=train_files, transform=train_transforms)
    # create a training data sampler
    train_sampler = DistributedSampler(dataset=train_ds, even_divisible=True, shuffle=True)
    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training
    train_loader = DataLoader(
        train_ds,
        batch_size=10,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        sampler=train_sampler,
    )

    # create UNet, DiceLoss and Adam optimizer
    device = torch.device(f"cuda:{args.local_rank}")
    torch.cuda.set_device(device)
    model = monai.networks.nets.UNet(
        spatial_dims=3,
        in_channels=1,
        out_channels=1,
        channels=(16, 32, 64, 128, 256),
        strides=(2, 2, 2, 2),
        num_res_units=2,
    ).to(device)
    loss_function = monai.losses.DiceLoss(sigmoid=True).to(device)
    optimizer = torch.optim.Adam(model.parameters(), 1e-3)
    # wrap the model with DistributedDataParallel module
    model = DistributedDataParallel(model, device_ids=[device], output_device=[device])

    # start a typical PyTorch training
    epoch_loss_values = list()
    for epoch in range(5):
        print("-" * 10)
        print(f"epoch {epoch + 1}/{5}")
        model.train()
        epoch_loss = 0
        step = 0
        train_sampler.set_epoch(epoch)
        for batch_data in train_loader:
            step += 1
            inputs, labels = batch_data["img"].to(device), batch_data["seg"].to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            epoch_len = len(train_ds) // train_loader.batch_size
            print(f"{step}/{epoch_len}, train_loss: {loss.item():.4f}")
        epoch_loss /= step
        epoch_loss_values.append(epoch_loss)
        print(f"epoch {epoch + 1} average loss: {epoch_loss:.4f}")
    print(f"train completed, epoch losses: {epoch_loss_values}")
    if dist.get_rank() == 0:
        # all processes should see same parameters as they all start from same
        # random parameters and gradients are synchronized in backward passes,
        # therefore, saving it in one process is sufficient
        torch.save(model.state_dict(), "final_model.pth")
    dist.destroy_process_group()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--dir", default="./testdata", type=str, help="directory to create random data")
    # must parse the command-line argument: ``--local_rank=LOCAL_PROCESS_RANK``, which will be provided by DDP
    parser.add_argument("--local_rank", type=int, default=0)
    args = parser.parse_args()

    train(args=args)

if __name__ == "__main__":
    main()

```

å®Œæ•´ä»£ç æ¥è‡ª [MONAI DDPæ•™ç¨‹](https://github.com/Project-MONAI/tutorials/blob/main/acceleration/distributed_training/unet_training_ddp.py)

##### step 8: ç»ˆç«¯å¯åŠ¨DDPè®­ç»ƒ

è¿™é‡Œçš„å¯åŠ¨æ–¹å¼é€‰æ‹©[torch.distributed.launch](https://pytorch.org/docs/stable/distributed.html#launch-utility)

ä½¿ç”¨è¯¥æ¨¡å—å¯åŠ¨è®­ç»ƒï¼Œåˆ†å•æœºå¤šå¡å’Œå¤šæœºå¤šå¡

- Single-Node multi-processï¼ˆå•æœºå¤šå¡ï¼‰

```bash
python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)
```

è¿™é‡Œä½¿ç”¨å•æœºï¼Œå› æ­¤èŠ‚ç‚¹nnodes=1ï¼ˆå¯ä»¥ç†è§£ä¸ºå‡ å°æœåŠ¡å™¨ï¼‰, `nproc_per_node`å‚æ•°æŒ‡å®šèŠ‚ç‚¹ä¸Šæœ‰å¤šå°‘å—GPUå¯ç”¨ã€‚ä¾‹å¦‚æœåŠ¡å™¨ä¸Šæœ‰2å—GPUï¼Œ`nproc_per_node=2`ã€‚å‡å¦‚æœ‰8å—ï¼Œæˆ‘åªæƒ³ç”¨å…¶ä¸­çš„4å—ã€‚åˆ™å¯ä»¥é€šè¿‡2ç§æ–¹å¼æŒ‡å®šï¼ˆå¦‚ä½•æŒ‡å®šGPU,è¯¦è§ä¸Šæ–‡ DP(DataParallel)éƒ¨åˆ†ï¼‰
```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 train.py -d "./testdata"
```

- Multi-Node multi-process(å¤šæœºå¤šå¡)

```bash
python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=0 --master_addr="192.168.1.1" --master_port=1234 
YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)
```

ä½¿ç”¨å‡ å°æœºå™¨ï¼Œnnodeså°±ç­‰äºå‡ ã€‚å¦å¤–ï¼Œéœ€è¦æ·»åŠ `master_addr`å’Œ`master_port`ä¸¤ä¸ªå‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„IPå’Œç«¯å£å·ã€‚è¿™ä¸¤ä¸ªå‚æ•°çš„å…·ä½“ä½¿ç”¨å¯é€šè¿‡ä¸‹é¢å‘½ä»¤æŸ¥çœ‹ï¼Œè¯¥æ¨¡å—çš„æ›´å¤šå¯é€‰å‚æ•°ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹

```bash
python -m torch.distributed.launch --help
```

ä»¥ä¸Šå°±æ˜¯ä½¿ç”¨DDPè®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚å¦‚æœé¡ºåˆ©çš„è¯ï¼ŒãŠ—ï¸ç¦ä½ ï¼Œyou are so lucky.



### ç»éªŒæ€»ç»“

ä»¥ä¸‹æ˜¯ä¸€äº›ç»éªŒæ€»ç»“ï¼Œå¤±è´¥çš„æœ‹å‹ç»§ç»­å¾€ä¸‹çœ‹ğŸ˜‚

#### æ³¨æ„ä¸€ï¼šä½¿ç”¨åˆ†å¸ƒå¼éœ€è®¾ç½®set_epoch

åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ï¼Œéœ€è¦åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶è°ƒç”¨set_epochæ–¹æ³•ï¼Œç„¶åå†åˆ›å»ºDataloaderè¿­ä»£å™¨ï¼Œä»¥ä½¿shuffleç”Ÿæ•ˆï¼Œå¦åˆ™ï¼Œæ¯ä¸ªepochä¸­ï¼Œé€‰æ‹©æ•°æ®çš„é¡ºåºä¸ä¼šå˜ã€‚å‚è€ƒ pytorch DistributedSampler å®˜æ–¹ä»£ç 

æ­£ç¡®ä½¿ç”¨å§¿åŠ¿
```python
sampler = DistributedSampler(dataset) if is_distributed else None
loader = DataLoader(dataset, shuffle=(sampler is None),
                    sampler=sampler)
for epoch in range(start_epoch, n_epochs):
    if is_distributed:
        sampler.set_epoch(epoch)
    train(loader)

```

æˆ‘ä»¬å¯ä»¥åšä¸ªå®éªŒï¼ŒéªŒè¯ä¸€ä¸‹æ˜¯ä¸æ˜¯è¿™æ ·ã€‚

å…ˆåˆ›å»ºä¸€ä¸ªéšæœºdataï¼Œå¹¶ä½¿ç”¨samplerè¿›è¡Œé‡‡æ ·ï¼Œå†é€å…¥dataloader

```python
import torch
from torch.utils.data import Dataset, DataLoader


class RandomDataset(Dataset):
    def __init__(self):
        self.data = torch.randn(21, 1, 32, 32)
        self.name = torch.arange(1, 22)  # è¿™é‡Œä»1å¼€å§‹ï¼Œ æ‰€æœ‰æ˜¯22ï¼Œä¸æ˜¯21

    def __getitem__(self, idx):
        return self.name[idx], self.data[idx]

    def __len__(self):
        return len(self.data)


torch.distributed.init_process_group(backend='nccl')

dataset = RandomDataset()
sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=True)
dataloader = DataLoader(dataset,
                        batch_size=5,
                        drop_last=True,
                        sampler=sampler)

```

å®éªŒä¸€ï¼šä¸ä½¿ç”¨`set_epoch`

```python
for epoch in range(3):
    print(f'epoch: {epoch}')
    # sampler.set_epoch(epoch)  # æ³¨é‡Šæ‰è¿™è¡Œ
    for i, data in enumerate(dataloader, 0):
        names, _ = data
        print(names)
```

![image-20241225172145987](æ¨¡å‹è®­ç»ƒ.assets/image-20241225172145987.png)

ä»ç»“æœå¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªepochä½¿ç”¨çš„dataæ•°æ®é¡ºåºéƒ½æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ï¼Œå¹¶æ²¡æœ‰shuffle

å®éªŒäºŒï¼šä½¿ç”¨`set_epoch`

```python
for epoch in range(3):
    print(f'epoch: {epoch}')
    sampler.set_epoch(epoch)  # æ³¨é‡Šæ‰è¿™è¡Œ
    for i, data in enumerate(dataloader, 0):
        names, _ = data
        print(names)
```

![image-20241225172233579](æ¨¡å‹è®­ç»ƒ.assets/image-20241225172233579.png)

ä»å›¾ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨`set_epoch`åshuffleèµ·ä½œç”¨äº†ã€‚

#### æ³¨æ„äºŒï¼šç›¸å…³æ¦‚å¿µè¯´æ˜

- rankï¼šç”¨äºè¡¨ç¤ºè¿›ç¨‹çš„ç¼–å·/åºå·ï¼ˆåœ¨ä¸€äº›ç»“æ„å›¾ä¸­rankæŒ‡çš„æ˜¯è½¯èŠ‚ç‚¹ï¼Œrankå¯ä»¥çœ‹æˆä¸€ä¸ªè®¡ç®—å•ä½ï¼‰ï¼Œæ¯ä¸€ä¸ªè¿›ç¨‹å¯¹åº”äº†ä¸€ä¸ªrankçš„è¿›ç¨‹ï¼Œæ•´ä¸ªåˆ†å¸ƒå¼ç”±è®¸å¤šrankå®Œæˆã€‚
- nodeï¼šç‰©ç†èŠ‚ç‚¹ï¼Œå¯ä»¥æ˜¯ä¸€å°æœºå™¨ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå®¹å™¨ï¼ŒèŠ‚ç‚¹å†…éƒ¨å¯ä»¥æœ‰å¤šä¸ªGPUã€‚
- rankä¸local_rankï¼š rankæ˜¯æŒ‡åœ¨æ•´ä¸ªåˆ†å¸ƒå¼ä»»åŠ¡ä¸­è¿›ç¨‹çš„åºå·ï¼›local_rankæ˜¯æŒ‡åœ¨ä¸€ä¸ªnodeä¸Šè¿›ç¨‹çš„ç›¸å¯¹åºå·ï¼Œlocal_rankåœ¨nodeä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚
- nnodesã€node_rankä¸nproc_per_nodeï¼š nnodesæ˜¯æŒ‡ç‰©ç†èŠ‚ç‚¹æ•°é‡ï¼Œnode_rankæ˜¯ç‰©ç†èŠ‚ç‚¹çš„åºå·ï¼›nproc_per_nodeæ˜¯æŒ‡æ¯ä¸ªç‰©ç†èŠ‚ç‚¹ä¸Šé¢è¿›ç¨‹çš„æ•°é‡ã€‚
- word size ï¼š å…¨å±€ï¼ˆä¸€ä¸ªåˆ†å¸ƒå¼ä»»åŠ¡ï¼‰ä¸­ï¼Œrankçš„æ•°é‡ã€‚
  [æ¦‚å¿µè§£æ](https://zhuanlan.zhihu.com/p/358974461)

#### æ³¨æ„ä¸‰ï¼šmodelå…ˆto(device)å†DDP

ç”¨ DistributedDataParallel åŒ…è£¹æ¨¡å‹ä¹‹å‰éœ€è¦å…ˆå°†æ¨¡å‹é€åˆ°deviceä¸Šï¼Œä¹Ÿå°±æ˜¯è¦å…ˆé€åˆ°GPUä¸Šï¼Œå¦åˆ™ä¼šæŠ¥é”™[åŸæ–‡é“¾æ¥](https://blog.csdn.net/laizi_laizi/article/details/115299263)ï¼š`AssertionError: DistributedDataParallel device_ids and output_device arguments only work with single-device GPU modules, but got device_ids [1], output_device 1, and module parameters {device(type='cpu')}.`


#### æ³¨æ„å››ï¼šbatchsizeå«ä¹‰æœ‰åŒºåˆ«

äºDPè€Œè¨€ï¼Œè¾“å…¥åˆ°dataloaderé‡Œé¢çš„batch_sizeå‚æ•°æŒ‡çš„æ˜¯æ€»çš„batch_sizeï¼Œä¾‹å¦‚batch_size=30ï¼Œä½ æœ‰ä¸¤å—GPUï¼Œåˆ™æ¯å—GPUä¼šåƒ15ä¸ªsampleï¼›å¯¹äºDDPè€Œè¨€ï¼Œé‡Œé¢çš„batch_sizeå‚æ•°æŒ‡çš„å´æ˜¯æ¯ä¸ªGPUçš„batch_sizeï¼Œä¾‹å¦‚batch_size=30ï¼Œä½ æœ‰ä¸¤å—GPUï¼Œåˆ™æ¯å—GPUä¼šåƒ30ä¸ªsampleï¼Œä¸€ä¸ªbatchæ€»å…±å°±åƒ60ä¸ªsample.

å…³äºè¿™ç‚¹ï¼Œä½¿ç”¨ä¸Šé¢çš„DPå’ŒDDPä»£ç ï¼Œçœ‹ä¸‹é¢è¿™å¥çš„è¿è¡Œç»“æœï¼Œå°±çŸ¥é“äº†
```python
print(f"{step}/{epoch_len}
```

ä½ ä¼šå‘ç°ï¼Œåœ¨ä½¿ç”¨DDPæ—¶ï¼Œå‡å¦‚epoch_len=40ï¼Œä¸¤å—GPUè¿›è¡Œè®­ç»ƒï¼Œstepæœ€å¤šç­‰äº20ï¼Œä¸ä¼šç­‰äº40ã€‚

#### åŸºäºhorovodçš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•

é™¤äº† Pytorch åŸç”Ÿæä¾›çš„ DP å’Œ DDP æ–¹å¼ä»¥å¤–ï¼Œä¹Ÿæœ‰å¾ˆå¤šä¼˜ç§€çš„ç”±ç¬¬ä¸‰æ–¹æä¾›çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ï¼Œå…¶ä¸­ Horovod å°±æ˜¯æ¯”è¾ƒå¸¸ç”¨çš„ä¸€æ¬¾ã€‚

Horovod æ˜¯ Uber å¼€æºçš„è·¨å¹³å°åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼ˆhorovod åå­—æ¥æºäºä¿„ç½—æ–¯ä¸€ç§æ°‘é—´èˆè¹ˆï¼Œèˆè€…æ‰‹æ‹‰æ‰‹ç«™æˆä¸€ä¸ªåœ†åœˆè·³èˆï¼Œç±»æ¯”äº† GPU è®¾å¤‡ä¹‹é—´çš„é€šä¿¡æ¨¡å¼-ã€‚

Horovod é‡‡ç”¨ all-reduce æ¶æ„æ¥æé«˜åˆ†å¸ƒå¼è®¾å¤‡çš„é€šä¿¡æ•ˆç‡ã€‚åŒæ—¶ï¼ŒHorovod ä¸ä»…æ”¯æŒ Pytorchï¼Œä¹Ÿæ”¯æŒ TensorFlow ç­‰å…¶ä»–æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚

DDP å’Œ horovodæ˜¯ç›®å‰æ¯”è¾ƒè®¤å¯çš„2ç§æ–¹æ³•ã€‚ä½†æ˜¯æˆ‘åœ¨å®‰è£…horovodæ˜¯å¤±è´¥äº†ï¼Œæ²¡æ³•è¿›è¡Œåé¢çš„æµ‹è¯„å•¦ã€‚æ„Ÿå…´è¶£çš„å‚è€ƒä»¥ä¸‹é“¾æ¥

[install horovod](https://github.com/horovod/horovod/blob/master/docs/gpus.rst)

[MONAI horovod demo](https://github.com/Project-MONAI/tutorials/blob/main/acceleration/distributed_training/unet_training_horovod.py)



